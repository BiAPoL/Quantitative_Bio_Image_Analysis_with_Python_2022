{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f78b35b",
   "metadata": {},
   "source": [
    "# UMAP\n",
    "## Uniform Manifold Approximation and Projection for Dimension Reduction\n",
    "\n",
    "Source material:   \n",
    "Tutorial: https://umap-learn.readthedocs.io/en/latest/  \n",
    "Paper: https://arxiv.org/abs/1802.03426 \n",
    "\n",
    "Notebook adapted from Anna Poetsch ([source](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/tree/main/10_correlation_dim_reduction)) under [CC-BY-4.0](https://github.com/BiAPoL/Bio-image_Analysis_with_Python/blob/main/LICENSE-CC-BY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fb52b",
   "metadata": {},
   "source": [
    "Packages (if not available, pip install):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed0c95d",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaa0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c24ba",
   "metadata": {},
   "source": [
    "Show data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a96094",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_array = plt.subplots(20, 20)\n",
    "axes = ax_array.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray_r')\n",
    "plt.setp(axes, xticks=[], yticks=[], frame_on=False)\n",
    "plt.tight_layout(h_pad=0.5, w_pad=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd54c6-5706-4922-bfdc-62532fa1e0c6",
   "metadata": {},
   "source": [
    "As you can see these are quite low resolution images – for the most part they are recognisable as digits, but there are a number of cases that are sufficiently blurred as to be questionable even for a human to guess at. The zeros do stand out as the easiest to pick out as notably different and clearly zeros. Beyond that things get a little harder: some of the squashed thing eights look awfully like ones, some of the threes start to look a little like crossed sevens when drawn badly, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dad82f-b951-4609-8a4e-97ce4e4d0560",
   "metadata": {},
   "source": [
    "Each image can be unfolded into a 64 element long vector of grayscale values. It is these 64 dimensional vectors that we wish to analyse: how much of the digits structure can we discern? At least in principle 64 dimensions is overkill for this task, and we would reasonably expect that there should be some smaller number of “latent” features that would be sufficient to describe the data reasonably well. We can try a scatterplot matrix – in this case just of the first 10 dimensions so that it is at least plottable, but as you can quickly see that approach is not going to be sufficient for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a859a-6646-46d4-be86-1c08c15697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame(digits.data[:,1:11])\n",
    "digits_df['digit'] = pd.Series(digits.target).map(lambda x: 'Digit {}'.format(x))\n",
    "sns.pairplot(digits_df, hue='digit', palette='Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0836be3-db00-4390-83c4-d00bfcadefca",
   "metadata": {},
   "source": [
    "In contrast we can try using UMAP again. It works exactly as before: construct a model, train the model, and then look at the transformed data. To demonstrate more of UMAP we’ll go about it differently this time and simply use the `fit` method rather than the `fit_transform` approach we used for Penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926449c9-0ac2-447e-8b84-0790f2d98d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b2588-404f-4db0-bccb-551104b5f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cef83a-9d85-4f20-9865-4a10f5b0e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.UMAP(a=None, angular_rp_forest=False, b=None,\n",
    "     force_approximation_algorithm=False, init='spectral', learning_rate=1.0,\n",
    "     local_connectivity=1.0, low_memory=False, metric='euclidean',\n",
    "     metric_kwds=None, min_dist=0.1, n_components=2, n_epochs=None,\n",
    "     n_neighbors=15, negative_sample_rate=5, output_metric='euclidean',\n",
    "     output_metric_kwds=None, random_state=42, repulsion_strength=1.0,\n",
    "     set_op_mix_ratio=1.0, spread=1.0, target_metric='categorical',\n",
    "     target_metric_kwds=None, target_n_neighbors=-1, target_weight=0.5,\n",
    "     transform_queue_size=4.0, transform_seed=42, unique=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7250848b-c93a-44ce-9971-ca2464f3d254",
   "metadata": {},
   "source": [
    "Now, instead of returning an embedding we simply get back the reducer object, now having trained on the dataset we passed it. To access the resulting transform we can either look at the embedding_ attribute of the reducer object, or call transform on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0af09-2a65-4967-bae5-5511ed18bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reducer.transform(digits.data)\n",
    "# Verify that the result of calling transform is\n",
    "# idenitical to accessing the embedding_ attribute\n",
    "assert(np.all(embedding == reducer.embedding_))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f272a6-944a-4638-b222-b9bed972b1e3",
   "metadata": {},
   "source": [
    "We now have a dataset with 1797 rows (one for each hand-written digit sample), but only 2 columns. As with the Penguins example we can now plot the resulting embedding, coloring the data points by the class that they belong to (i.e. the digit they represent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435f0cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66dd37-3429-4cce-a4b7-477741832238",
   "metadata": {},
   "source": [
    "We see that UMAP has successfully captured the digit classes. There are also some interesting effects as some digit classes blend into one another (see the eights, ones, and sevens, with some nines in between), and also cases where digits are pushed away as clearly distinct (the zeros on the right, the fours at the top, and a small subcluster of ones at the bottom come to mind). To get a better idea of why UMAP chose to do this it is helpful to see the actual digits involve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
